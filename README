To start a Registry:
    gradle registry -Pargv="['<port>']"

To start a Grid Scheduler:
    gradle gs -Pargv="['<gsName>', '<registryHost>', '<registryPort>' [, '<peerGs1>', ... , <peerGsN>']]"

To start a Frontend:
    gradle frontend -Pargv="['<name>', '<registryHost>', '<registryPort>', '<gridScheduler>']"

To start a RM / Cluster:
    gradle rm -Pargv="['<rmName>', '<numNodes>', '<registryHost>', '<registryPort>', '<gsName>']"

To start a Client:
    gradle client -Pargv="['<name>', '<numJobs>', 'minJobDuration', 'maxJobDuration', '<registryHost>', '<registryPort>', '<rmName>']"

or alternatively (extra options available, see --help):
    python launch_jobs.py <numjobs> <resc mngr>


For example, starting up the system could look like this:

    # Start a registry
    $ gradle registry -Pargv="['1099']"

    # Start the first GS. Note this one takes no further GS names.
    $ gradle gs -Pargv="['gs1', '127.0.0.1', '1099']"

    # Start other GS. You have to provide at least one name of a GS, from which all other GSs can be requested.
    $ gradle gs -Pargv="['gs2', '127.0.0.1', '1099', 'gs1']"
    $ gradle gs -Pargv="['gs3', '127.0.0.1', '1099', 'gs1']"

    # Start a frontend that will monitor all events logged by any of the GSs.
    $ gradle frontend -Pargv="['frontend1', '127.0.0.1', '1099', 'gs1']"

    # Start a number of RMs
    $ gradle rm -Pargv="['rm1', '10', '127.0.0.1', '1099', 'gs1']"
    $ gradle rm -Pargv="['rm2', '10', '127.0.0.1', '1099', 'gs1']"
    $ gradle rm -Pargv="['rm3', '10', '127.0.0.1', '1099', 'gs2']"
    $ gradle rm -Pargv="['rm4', '10', '127.0.0.1', '1099', 'gs3']"

    # Start a number of clients
    $ gradle client -Pargv="['client1', '5', '5000', '14000', '127.0.0.1','1099', 'rm1']"
    $ gradle client -Pargv="['client2', '5', '5000', '14000', '127.0.0.1','1099', 'rm2']"
    $ gradle client -Pargv="['client3', '5', '5000', '14000', '127.0.0.1','1099', 'rm3']"
    $ gradle client -Pargv="['client4', '5', '5000', '14000', '127.0.0.1','1099', 'rm4']"
    $ gradle client -Pargv="['client5', '5', '5000', '14000', '127.0.0.1','1099', 'rm4']"

To convert a Grid Workload Format file to Time Inclined Model format,
the "test_dataset.py" script can be used.
./test_dataset.py (or python test_dataset.py making sure you use python2.7)
To convert a dataset makesure it is in the correct directory structure:
datasets/[DATASET_NAME]/anon_jobs.gwf
then after running the script select the dataset you want to convert by entering it's index as a number.
e.g.:

Starting test conversion
The available datasets are:
0: AuverGrid 
1: DAS2 

Select a dataset index as input[0]: 

We could select 0 for AuverGrid and 1 for DAS2, in case no number is given the default is dataset 0.
The output is then written to input.tim


To run the automated tests we use the launch_tim.py script.
For running local running and one for running on the DAS
Which is written in python3.
To run it you can use ./launch_tim.py -h (or python3 launch_tim.py -h)
refer to the scripts help section for it's arguments.

For example, to run on the DAS we use the command:
prun -v -np 1 launch_tim.py -g $i:$i:$i:$i:$i -j 2000 > "cluster$i.log" &
